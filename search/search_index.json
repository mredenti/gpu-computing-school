{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"gpu-computing-school","text":""},{"location":"KOKKOS/","title":"Building Kokkos inline","text":"<p>For the tutorial, we will compile our Kokkos programs via a Makefile while building Kokkos inline. This allows us to easily swap between different default execution spaces and memory spaces.</p> Instructions: Cloning Kokkos Core repository <p>Change into your work area on Leonardo... <pre><code>cd $WORK</code></pre></p> <p>...define Kokkos release version/tag to clone... <pre><code>export KOKKOS_TAG=4.1.00</code></pre></p> <p>...clone the Kokkos repository... <pre><code>git clone --branch $KOKKOS_TAG https://github.com/kokkos/kokkos.git kokkos-$KOKKOS_TAG </code></pre></p> <p>...and finally export the path to the Kokkos folder. <pre><code>export KOKKOS_PATH=$PWD/kokkos-$KOKKOS_TAG</code></pre></p> Tip <p>To avoid having to export this environment variable every time you open a new shell, you might want to add it to your <code>~/.bashrc</code> file</p> Installing Kokkos as shared library/package <p>You may consult the documentation to learn about:   Building Kokkos as an intalled package Building Kokkos via Spack package manager but for the tutorial we will compile Kokkos programs inline via a Makefile</p> <p>Next</p> <p>Unless you have already done so </p> Clone repository with SYCL and KOKKOS exercises <pre><code>git clone https://gitlab.hpc.cineca.it/amasini0/gpu-computing-sycl-kokkos.git</code></pre> <p>Great! We can now turn to running our first Kokkos program Tutorial 01: Vector Addition</p>"},{"location":"KOKKOS/matVecMul/","title":"Exercise 1: Matrix-Vector Multiplication","text":"<p>This exercise asks you to investigate of the performance of matrix-vector multiplication. More precisely, you will witness how performance varies with the shape of the matrix while keeping the overall size fixed.</p> Step 0: Navigate to the <code>matVecMul</code> directory <pre><code>cd gpu-computing-sycl-kokkos/KOKKOS/matVecMul/begin</code></pre> Step 1: Complete the matrix vector multiplication program <p>The program is partially implemented, and you will need to complete the sections marked with <code>@TASK@</code>.</p> Step 2: Investigate the performance as you vary the shape of the matrix <p>You can compile the program using the Makefile provided. </p> <p>Compile with CUDA support</p> <pre><code>make BACKEND=cuda</code></pre> <p>Launch an interactive session and run the matrix vector multiplication varying the number of rows N while keeping constant the size S of the matrix</p> <p><pre><code>srun -X -t 10 -N 1 -n 1 --mem=8GB --gres=gpu:1 -p boost_usr_prod -A tra23_advgpu --reservation s_tra_advgpu --qos=qos_lowprio --pty /usr/bin/bash</code></pre> <pre><code>./matVecMul.cudax -N 12</code></pre> <pre><code>./matVecMul.cudax -N 5</code></pre></p> Question <p>What do you note about the execution time and/or the bandwidth?</p> Task <p>Take a note of the bandwidths or the execution time. You will compare them to the ones obtained in the next exercise when you will re-write the Matrix Vector Multiplication kernel using team policies</p> Next <p>Great! We can now turn to the next exercise Tutorial 03: Matrix Vector Multiplication + Team Policy</p>"},{"location":"KOKKOS/matVecMulTeamPolicy/","title":"Exercise 2: Matrix-Vector Multiplication + Team Policy","text":"<p>This exercise asks you to improve the performance of the matrix-vector multiplication for rectangular matrices using hierarchical parallelism.</p> Step 0: Navigate to the <code>matVecMulTeamPolicy</code> directory <pre><code>cd gpu-computing-sycl-kokkos/KOKKOS/matVecMulTeamPolicy/begin</code></pre> Step 1: Complete the matrix vector multiplication program <p>The starting point is the matrix vector multiplication program that you completed previously.  Now, you have to re-write it using Team and TeamThreadRange policies. The section of interest is marked by <code>TASK@</code>.</p> Step 2: Compare the performance to the previous version of the matrix vector multiplication kernel <p>Compile with CUDA support</p> <pre><code>make BACKEND=cuda</code></pre> <p>Launch an interactive session and run with the same parameters as before</p> <p><pre><code>srun -X -t 10 -N 1 -n 1 --mem=8GB --gres=gpu:1 -p boost_usr_prod -A tra23_advgpu --reservation s_tra_advgpu --qos=qos_lowprio --pty /usr/bin/bash</code></pre> <pre><code>./matVecMulTeamPolicy.cudax -N 12</code></pre> <pre><code>./matVecMulTeamPolicy.cudax -N 5</code></pre></p> Question <p>Do you note an improvement in the execution time and/or the bandwidth?</p> Question <p>Think about memory access patterns. Do you think the default layout is optimal for this version of the matrix vector multiplication kernel? If not, can you explain why?</p> Task <p>Try change the layout from <code>Kokkos::LayoutLeft</code> to <code>Kokkos::LayoutRight</code>.  Then, re-compile the program </p> <pre><code>make BACKEND=cuda</code></pre> Danger <p>Make sure to be running the example on a compute node</p> <p>and run with the same parameters </p> <p><pre><code>./matVecMulTeamPolicy.cudax -N 12</code></pre> <pre><code>./matVecMulTeamPolicy.cudax -N 5</code></pre></p> Question <p>Do you observe any improvement?</p>"},{"location":"KOKKOS/vectorAdd/","title":"Exercise 0: Vector Addition","text":"<p>This guide will walk you through the process of compiling and running a vector addition program using Kokkos.  The program demonstrates basic usage of Kokkos for heterogeneous computing, including the use of different execution backends.</p> Step 0: Navigate to the <code>vectorAdd</code> directory <pre><code>cd gpu-computing-sycl-kokkos/KOKKOS/vectorAdd/begin</code></pre> Step 1: Complete the vector addition program <p>The program is partially implemented, and you will need to complete the sections marked with <code>@TASK@</code>.</p> Step 2: Compile and Run <p>You can compile the program using the Makefile provided. </p> Serial compilation and execution <p>By default, the Makefile compiles for serial execution. To compile the program, simply run:</p> <pre><code>make</code></pre> <p>The compilation step should give you yield an executable named <code>vecAdd.serialx</code></p> <p>To run the serial Kokkos version of the vector addition program launch an interactive session and run the program </p> <pre><code>srun -X -t 10 -N 1 -n 1 --mem=8GB --gres=gpu:1 -p boost_usr_prod -A tra23_advgpu --reservation s_tra_advgpu --qos=qos_lowprio --pty /usr/bin/bash\n./vecAdd.serialx</code></pre> Cuda compilation and execution <p>Kokkos supports various backends such as CUDA for GPUs. To compile the program for a specific backend, you can pass the BACKEND variable to Make. </p> <p>For example, to compile with CUDA support, use:</p> <pre><code>make BACKEND=cuda</code></pre> Question <p>What do you note different about the compilation w.r.t. the serial compilation?</p> <p>The compilation step should give you yield an executable named <code>vecAdd.cudax</code></p> <p>Launch an interactive session and run the program </p> <pre><code>srun -X -t 10 -N 1 -n 1 --mem=8GB --gres=gpu:1 -p boost_usr_prod -A tra23_advgpu --reservation s_tra_advgpu --qos=qos_lowprio --pty /usr/bin/bash\n./vecAdd.cudax</code></pre> Optional: Comparing performances against Cuda/SYCL version <ul> <li>Implement your own version of vector addition in CUDA (perhaps retrieve one from the exercises during the school)</li> <li>Time the execution in a similar manner as for the Kokkos vector program and compare the execution times</li> </ul> <p>You may just compare performance to a SYCL version as well</p> Next <p>Great! We can now turn to our next exercise Tutorial 02: Matrix Vector Multiplication</p>"},{"location":"SYCL/","title":"Setup for Running the Exercises","text":""},{"location":"SYCL/#installing-sycl","title":"Installing SYCL","text":"<p>In order to compile the code for the exercises we need to have SYCL installed on our machine. There are various SYCL distributions that can be used for this purpose, but it is definitely easier if we stick with one of the main three: </p> <ul> <li>Data Parallel C++ (DPC++): Intel's implentation of SYCL, distributed as part of their oneAPI Base Toolkit.  </li> <li>AdaptiveCpp: An independent SYCL implementation, supporting a multitude of compilation flows. See here for more info.</li> <li>ComputeCpp: Based on SYCL 1.2.1 (not SYCL2020), developed by Codeplay. Has native support for NVIDIA GPUs.</li> </ul> <p>Detailed instructions on how to install them can be found on the corresponding product pages (linked). DPC++ and ComputeCpp are provided as binaries (in the form of a SYCL-enabled compiler), while AdaptiveCpp has to be compiled from source.</p> <p>Intel's DPC++ is the easiest to get (at least for SYCL2020). AdaptiveCpp is very flexible and has more backends available, but it is also not-so-easy to compile, depending on the compilation flow you want, and the software dependencies it requires.</p>"},{"location":"SYCL/#tutorial-environment-on-leonardo-hpc-cluster","title":"Tutorial environment on Leonardo HPC Cluster","text":"<p>If you are compiling these exercises on Leonardo, you can use the <code>setenv-leo.sh</code> script in the <code>SYCL</code> folder inside the repository to set up the environment required for the compiler to work.</p> <p>The compiler is Intel's <code>icpx</code> found in the oneAPI Toolkit. In order to make it work also on NVIDIA GPUs (natively it only works with Intel's GPUs) there is a free plugin maintained by Codeplay. The page for the plugin, with links for the download and a detailed guid on how to use it, can be found at the following link.</p> <p>The following commands will clone the repo and acivate the environment <pre><code># clone the repository\ngit clone https://gitlab.hpc.cineca.it/amasini0/gpu-computing-sycl-kokkos.git\n\n# enter the SYCL folder\ncd gpu-computing-sycl-kokkos/SYCL\ncd SYCL\n\n# source the setenv file\n. setenv-leo.sh</code></pre></p> <p>If you did everything correclty you should see a message on the terminal that says that a module has been loaded. This script loads the <code>oneapi/compiler/latest</code> module and its dependencies, together with the <code>nvhpc/23.5</code> module, required to compile the CUDA versions of the exercises that we will use as reference for the performance. Furthermore, the script also exports an environment variable required to allow the SYCL compiler to find the required CUDA libraries to run the code on the GPUs (this is only required since the CUDA Toolkit on the cluster is installed on non-standard locations.</p>"},{"location":"SYCL/#compiling-on-leonardo","title":"Compiling on Leonardo","text":"<p>The exercise can be compiled simply by running one of the following three commands in the corresponding folder.</p> <ul> <li><code>make</code>: to compile the SYCL code using CPU as backend (executes only on CPU)</li> <li><code>make BACKEND=cuda-leo</code>: to compile the SYCL code using CUDA backend (executes on Leonardo's GPUs).</li> <li><code>make TARGET=cuda</code>: to compile the CUDA code (executed on Leonardo's GPUs).</li> </ul> <p>For example, to compile the <code>globalMatMul</code> SYCL example, and run it on GPU, we can do the following: <pre><code>cd globalMatMul\nmake BACKEND=cuda-leo</code></pre> We should see the message <code>building globalMatMul_sycl_cuda-leo ...</code> appear on the terminal.</p>"},{"location":"SYCL/#compiling-on-other-machines","title":"Compiling on other machines","text":"<p>There is also another compilation target available, namely <code>make BACKEND=cuda</code>, which targets generic CUDA backend, but it requires all the software to be installed in standard locations, and targets the CUDA sm_50 architecture by default, to enable the code to run on the majority of NVIDIA GPUs. </p> <p>This target does not work on Leonardo (thus it has its own), but it can be useful when installing SYCL on your laptop (only Linux, only using distribution packages). This is not tested however.</p> <p>If these targets are not suitable for the your target machine, it suffices to add a fel lines to the <code>Makefile.in</code>, and set the required compilation flags for the SYCL compiler.</p>"},{"location":"SYCL/globalMatMul/","title":"Global Matrix Multiplication","text":"<p>In this exercise you are going to implement the classic matrix multiplication using SYCL.</p> <p>The <code>exercise</code> folder contains the <code>globalMatMul.cpp</code> file, which is a partially filled template. You have to complete it with the code to perform the matrix multiplication, and the code to offload the kernel to the device. The spots that have to be filled are marked with a <code>//TODO</code> comment.</p> <p>The code in the template already initializes the matrices and performs the memory transfers required. Furthermore it checks the results of the multiplication and reports the error in output, together with some execution metrics. </p>"},{"location":"SYCL/globalMatMul/#compiling","title":"Compiling","text":"<p>In order to compile the exercise it is sufficient to run the following commands inside the exercise directory <pre><code># compile for CPU\nmake\n\n# compile for GPU\nmake BACKEND=cuda-leo</code></pre></p>"},{"location":"SYCL/globalMatMul/#running","title":"Running","text":"<p>In order to run the exercise on Leonardo using GPUs you need to request a GPU node <pre><code>srun -X -t 10 -N 1 -n 1 --mem=8GB --gres=gpu:1 -p boost_usr_prod -A tra23_advgpu --reservation s_tra_advgpu --qos=qos_lowprio --pty /usr/bin/bash\n./globalMatMul_sycl_cuda-leo</code></pre> The CPU version can be run on the login node, but the <code>SIZE</code> must be set to ad adequate value, otherwise the execution will be interrupted <pre><code># setting SIZE = 512\n# recompiling\n./globalMatMul_sycl_cpu</code></pre></p>"},{"location":"SYCL/globalMatMul/#compiling-and-running-the-solution","title":"Compiling and running the solution","text":"<p>The folder <code>solution</code> contains a solution to the exercise. The commands for compiling and running are the same, except that they must be run in the <code>solution</code> folder.</p> <p>There is also a CUDA version of the solution, that can be compiled and run as follows <pre><code># compile CUDA version\nmake TARGET=cuda\n\n# run on GPU node\nsrun -X -t 10 -N 1 -n 1 --mem=8GB --gres=gpu:1 -p boost_usr_prod -A tra23_advgpu --reservation s_tra_advgpu --qos=qos_lowprio --pty /usr/bin/bash\n./globalMatMul_cuda</code></pre></p>"},{"location":"SYCL/prefixSum/","title":"Parallel Prefix Sum","text":"<p>This example shows the implementation of a GPU-based prefix sum algorithm. The prefix-sum algorithm takes a vector, and for each entry of its entries computes the sum of all previous entries. </p> <p>The serial version of the algorith is extremely easy (it is a simple for loop), but it becomes much more complicated in its parallel version. A very good explanation of the parallel algorithm can be found at this link </p>"},{"location":"SYCL/prefixSum/#compiling","title":"Compiling","text":"<pre><code># enter the example directory\ncd prefixSum\n\n# SYCL CPU\nmake\n\n# SYCL GPU\nmake BACKEND=cuda-leo\n\n# CUDA\nmake TARGET=cuda</code></pre>"},{"location":"SYCL/prefixSum/#running","title":"Running","text":"<pre><code># run CPU version\n./prefixSum_sycl_cpu\n\n# run SYCL GPU version\n./prefixSum_sycl_cuda-leo\n\n# run CUDA GPU version\n./prefixSum_cuda</code></pre>"},{"location":"SYCL/prefixSum/#do-try-it-at-home","title":"Do Try it at Home!","text":"<p>If you want to try to implement your own version of this example, in the <code>extras</code> folder there are template files for SYCL and CUDA. The following instructions should allow you to compile your code without any problems: <pre><code># create a folder in the directory SYCL\nmkdir customKernel\n\n# copy the makefile from the original example\ncp prefixSum/Makefile customKernel/Makefile\n\n# copy the template\ncp extras/template.cpp custoKernel/customKernel.cpp\n\n# modify the template as you want\n# and compile for the required target of backend\nmake BACKEND=cuda-leo</code></pre> The templates already contain the code required to compute metrics for your kernel, if you wish to compare with the provided implementation.</p>"},{"location":"SYCL/stencilUpdate/","title":"2D Stencil Heat Map Update","text":"<p>In this exercise you will write a simple code that computes a stencil update to a 2D heat map. There is a source placed at <code>( SIZE/4, SIZE/4)</code>, where <code>SIZE</code> is the dimension of the stencil we are considering (i.e. the number of stencil nodes per side, assuming a square stencil).</p> <p>The update conditions are as follows:</p> <ul> <li>if we are at the source, i.e. <code>x,y = source_x, source_y</code> the value of the heat map is unchanged</li> <li>in any other point <code>(i,j)</code> it should be updated as follows: <pre><code>map[i,j] = (4 * map[i,j] + map[i-1,j] + map[i+1,j] + map[i,j-1] + map[i,j+1]) /8</code></pre></li> <li>if the point is on the border, the value of the map in the direction of the border is substituted with the value at the point: <pre><code>if (i==0)      map[i-1,j] =&gt; map[i,j]\nif (i==SIZE-1) map[i+1,j] =&gt; map[i,j]</code></pre></li> </ul> <p>The file <code>exercise/stencilUpdate.cpp</code> contains a partially compiled template for the exercise. You only need to fill the code for the kernel function, and the code to call it from host. The spots that have to be filled are marked with a <code>//TODO</code> comment. The templated code already sets up the memory on host and device, and performs the memory tranfers. Furthermore, it checks the result at the end, to ensure that the final results are consistent. The result of the check, together with some execution metrics are printed in the program output.</p>"},{"location":"SYCL/stencilUpdate/#compiling","title":"Compiling","text":"<p>In order to compile the exercise it is sufficient to run the following commands inside the exercise directory <pre><code># compile for CPU\nmake\n\n# compile for GPU\nmake BACKEND=cuda-leo</code></pre></p>"},{"location":"SYCL/stencilUpdate/#running","title":"Running","text":"<p>In order to run the exercise on Leonardo using GPUs you need to request a GPU node <pre><code>srun -X -t 10 -N 1 -n 1 --mem=8GB --gres=gpu:1 -p boost_usr_prod -A tra23_advgpu --reservation s_tra_advgpu --qos=qos_lowprio --pty /usr/bin/bash\n./stencilUpdate_sycl_cuda-leo</code></pre> The CPU version can be run on the login node, but the <code>SIZE</code> must be set to ad adequate value, otherwise the execution will be interrupted <pre><code># setting SIZE = 512\n# recompiling\n./stencilUpdate_sycl_cpu</code></pre></p>"},{"location":"SYCL/stencilUpdate/#compiling-and-running-the-solution","title":"Compiling and running the solution","text":"<p>The folder <code>solution</code> contains a solution to the exercise. The commands for compiling and running are the same, except that they must be run in the <code>solution</code> folder.</p> <p>There is also a CUDA version of the solution that can be compiled and run as follows <pre><code># compile CUDA version\nmake TARGET=cuda\n\n# run on GPU node\nsrun -X -t 10 -N 1 -n 1 --mem=8GB --gres=gpu:1 -p boost_usr_prod -A tra23_advgpu --reservation s_tra_advgpu --qos=qos_lowprio --pty /usr/bin/bash\n./stencilUpdate_cuda</code></pre></p>"},{"location":"SYCL/tiledMatMul/","title":"Tiled Matrix Multiplication","text":"<p>This example shows a (very fancy) implementation of a tiled matrix multiplication algorithm. The performance of this algorithm is significantly increased with respect to the <code>globalMatMul</code> case. </p> <p>The code is based on this article, and the following GitHub repository.</p>"},{"location":"SYCL/tiledMatMul/#compiling","title":"Compiling","text":"<pre><code># enter the example directory\ncd tiledMatMul\n\n# SYCL CPU\nmake\n\n# SYCL GPU\nmake BACKEND=cuda-leo\n\n# CUDA\nmake TARGET=cuda</code></pre>"},{"location":"SYCL/tiledMatMul/#running","title":"Running","text":"<pre><code># run CPU version\n./tiledMatMul_sycl_cpu\n\n# run SYCL GPU version\n./tiledMatMul_sycl_cuda-leo\n\n# run CUDA GPU version\n./tiledMatMul_cuda</code></pre>"},{"location":"SYCL/tiledMatMul/#do-try-it-at-home","title":"Do Try it at Home!","text":"<p>If you want to try to implement your own version of this example, in the <code>extras</code> folder there are template files for SYCL and CUDA. The following instructions should allow you to compile your code without any problems: <pre><code># create a folder in the directory SYCL\nmkdir customKernel\n\n# copy the makefile from the ooriginal example\ncp tiledMatMul/Makefile customKernel/Makefile\n\n# copy the template\ncp extras/template.cpp custoKernel/customKernel.cpp\n\n# modify the template as you want\n# and compile for the required target of backend\nmake BACKEND=cuda-leo</code></pre> The templates already contain the code required to compute metrics for your kernel, if you wish to compare with the provided implementation.</p>"},{"location":"docs/","title":"Home","text":"<p>This repository hosts a collection of SYCL and KOKKOS exercises for the upcoming School on GPU Computing, organized by Cineca.</p> Clone repository with SYCL and KOKKOS exercises <pre><code>git clone https://github.com/mredenti/gpu-computing-school.git</code></pre> <p>Next</p> <p>Start exploring heterogeneous programming with either SYCL or KOKKOS, you choose!</p>"},{"location":"docs/#sycl","title":"SYCL","text":"Name Description Setup: Getting SYCL and Tutorial Environment Info on how to get a SYCL compiler, and what we can use on Leonardo Exercise 1: 2D Heat Map Stencil Update Implementing a stencil update for a 2D heat map using SYCL Exercise 2: Global Matrix Multiplication Implementing global matrix multiplication using SYCL Example: Tiled Matrix Multiplication Example implementation of a tiled matrix multiplication algorithm Example: Parallel Prefix Sum Example implementation of a parallel prefix sum algorithm"},{"location":"docs/#kokkos","title":"Kokkos","text":"<p>This repository hosts a collection of Kokkos exercises for the upcoming School on GPU Computing, organized by Cineca.</p> Name Description Setup: Installing Kokkos and Tutorial Environment This step guides you through cloning the Kokkos Core library and setting up the tutorial environment. Exercise 0: Vector Addition Introduction to heterogeneous parallel programming Exercise 1: Matrix-Vector Multiplication Introduction to Kokkos parallel patterns Exercise 2: Matrix-Vector Multiplication With Teams Introduction to Kokkos parallel patterns"},{"location":"docs/#authors","title":"Authors","text":"<ul> <li>Michael Redenti and Alessandro Masini </li> </ul>"},{"location":"docs/#other-contributors","title":"Other contributors","text":"<p>See also the list of contributors who participated in this project.</p>"},{"location":"docs/#issues-feature-request","title":"Issues / Feature request","text":"<p>You can submit bug / issues / feature request using Tracker.</p>"},{"location":"docs/#license","title":"License","text":"<p>TBD</p>"}]}